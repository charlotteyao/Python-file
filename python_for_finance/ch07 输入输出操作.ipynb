{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ch07 输入/输出操作\n",
    "本章介绍如下领域：\n",
    "\n",
    "*基本I/O*\n",
    "    \n",
    "    **Python**有内建的函数，序列化对象、将其存储到磁盘、从磁盘将其读入RAM；除此之外，**Python**很擅长处理文本文件和**SQL**数据库。**Numpy**也提供了专用于快速存储和检索**ndarray**对象的函数。\n",
    " \n",
    "**pandas** *的I/O*\n",
    "\n",
    "    **pandas**库提供了丰富的便利函数及方法，读取不同格式存储的数据（例如csv、JSON），将数据写入不同格式的文件中。\n",
    "    \n",
    "**PyTables** *的I/O*\n",
    "\n",
    "    **PyTables**使用HDF5标准实现大数据集的快速I/O操作；其速度往往只受到所使用硬件的限制。\n",
    "## 7.1 Python基本I/O\n",
    "### 7.1.1 将对象写入磁盘\n",
    "使用**pickle**模块，可以序列化大部分**Python**对象，序列化指的是将对象（层次结构）转换为一个字节流；反序列化是相反的操作。下例中再次使用（伪）随机数据，这次保存在一个列表对象中："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path = 'D:/GitHub/python-study/python_for_finance/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from random import gauss\n",
    "a = [gauss(1.5,2) for i in range(1000000)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将列表对象写入磁盘供以后检索。**pickle**可以完成："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pkl_file = open(path + 'data.pkl','w')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "需要的两个重要函数是写入对象的**dump**和将对象加载到内存的**load**："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3.54 s\n"
     ]
    }
   ],
   "source": [
    "%time pickle.dump(a,pkl_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<open file 'D:/GitHub/python-study/python_for_finance/data.pkl', mode 'w' at 0x0000000009A2CC00>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pkl_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pkl_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "检查磁盘文件的大小，包含100万个浮点数的列表对象大约占据20兆字节（MB）磁盘空间："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#ll $path*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "有了磁盘数据，就可以通过**pickle.load**将其读入内存："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3.54 s\n"
     ]
    }
   ],
   "source": [
    "pkl_file = open(path+'data.pkl','r')\n",
    "%time b = pickle.load(pkl_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.684280862083098,\n",
       " 3.866452034479175,\n",
       " 1.909527931440512,\n",
       " 3.888071851306436,\n",
       " 6.013572682098312]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将此与原始对象的前5个浮点对象比较："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.684280862083098,\n",
       " 3.866452034479175,\n",
       " 1.909527931440512,\n",
       " 3.888071851306436,\n",
       " 6.013572682098312]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了确保对象a和b相同，**Numpy**提供了**allclose**函数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(np.array(a),np.array(b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "原理上，这和计算两个**ndarray**对象的差，检查是否为0一样："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(np.array(a)-np.array(b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然而，**allclose**有一个容错级别参数，默认设置为1e-5。\n",
    "\n",
    "用**pickle**存储和读取单个对象明显相当简单，那么两个对象呢？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.28 s\n"
     ]
    }
   ],
   "source": [
    "pkl_file = open(path + 'data.pkl','w')\n",
    "%time pickle.dump(np.array(a),pkl_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.21 s\n"
     ]
    }
   ],
   "source": [
    "%time pickle.dump(np.array(a)**2,pkl_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pkl_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上述操作：\n",
    "1. 将原始对象的**ndarray**版本写入磁盘；\n",
    "2. 还将**ndarray**的平方版本写入磁盘上的同一个文件；\n",
    "3. 两个操作都比原来的快（因为使用**ndarray**对象）；\n",
    "4. 文件的大小大致为以前的两倍；\n",
    "\n",
    "下面将两个**ndarray**对象读回内存，显然，**pickle**按照先进先出（FIFO）原则保存对象："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.68428086,  3.86645203,  1.90952793, ..., -1.115214  ,\n",
       "        3.41076313,  1.55253036])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pkl_file = open(path+'data.pkl','r')\n",
    "x = pickle.load(pkl_file)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0.4682403 ,  14.94945133,   3.64629692, ...,   1.24370227,\n",
       "        11.63330512,   2.41035053])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = pickle.load(pkl_file)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这种方法没有任何可用的元信息让用户事先知道保存在**pickle**文件中的是什么，可采用存储**字典（dict）**对象："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pkl_file = open(path+'data.pkl','w')\n",
    "pickle.dump({'x':x,'y':y},pkl_file)\n",
    "pkl_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "此方法可以一次读取整组对象，还有一些其他好处，例如可以在字典对象的关键值上循环："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y [  0.4682403   14.94945133   3.64629692  15.11710272]\n",
      "x [-0.68428086  3.86645203  1.90952793  3.88807185]\n"
     ]
    }
   ],
   "source": [
    "pkl_file = open(path+'data.pkl','r')\n",
    "data = pickle.load(pkl_file)\n",
    "pkl_file.close()\n",
    "for key in data.keys():\n",
    "    print key,data[key][:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "不过这种方法要求我们一次写入和读取所有对象。\n",
    "### 7.1.2 读写文本文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.3537,  0.285 ,  0.0062, -0.7421,  0.0045],\n",
       "       [ 0.5187,  0.1505, -0.5658,  0.2235,  0.6948],\n",
       "       [-0.2567, -0.4037, -0.4638,  0.0829, -0.9812],\n",
       "       ..., \n",
       "       [-0.1889, -1.1675, -0.1379,  0.9567, -0.295 ],\n",
       "       [ 0.3638, -0.4283, -0.8664,  0.7953,  0.9885],\n",
       "       [ 0.8206, -0.7244, -1.206 , -0.858 ,  1.6389]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows = 5000\n",
    "a = np.random.standard_normal((rows,5))\n",
    "a.round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatetimeIndex(['2014-01-01 00:00:00', '2014-01-01 01:00:00',\n",
       "               '2014-01-01 02:00:00', '2014-01-01 03:00:00',\n",
       "               '2014-01-01 04:00:00', '2014-01-01 05:00:00',\n",
       "               '2014-01-01 06:00:00', '2014-01-01 07:00:00',\n",
       "               '2014-01-01 08:00:00', '2014-01-01 09:00:00',\n",
       "               ...\n",
       "               '2014-07-27 22:00:00', '2014-07-27 23:00:00',\n",
       "               '2014-07-28 00:00:00', '2014-07-28 01:00:00',\n",
       "               '2014-07-28 02:00:00', '2014-07-28 03:00:00',\n",
       "               '2014-07-28 04:00:00', '2014-07-28 05:00:00',\n",
       "               '2014-07-28 06:00:00', '2014-07-28 07:00:00'],\n",
       "              dtype='datetime64[ns]', length=5000, freq='H')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "t = pd.date_range(start='2014/1/1',periods=rows,freq='H')\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "csv_file = open(path+'data.csv','w')\n",
    "header = 'date,no1,no2,no3,no4,no5\\n'\n",
    "csv_file.write(header)#写入数据列名称\n",
    "for t_,(no1,no2,no3,no4,no5) in zip(t,a):\n",
    "    s = '%s,%f,%f,%f,%f,%f\\n' % (t_,no1,no2,no3,no4,no5)\n",
    "    csv_file.write(s)\n",
    "csv_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "date,no1,no2,no3,no4,no5\n",
      "\n",
      "2014-01-01 00:00:00,-0.353731,0.285017,0.006235,-0.742147,0.004483\n",
      "\n",
      "2014-01-01 01:00:00,0.518702,0.150504,-0.565756,0.223530,0.694779\n",
      "\n",
      "2014-01-01 02:00:00,-0.256681,-0.403742,-0.463805,0.082921,-0.981190\n",
      "\n",
      "2014-01-01 03:00:00,0.669372,0.390002,0.020442,0.221858,1.088725\n",
      "\n"
     ]
    }
   ],
   "source": [
    "csv_file = open(path + 'data.csv','r')\n",
    "for i in range(5):\n",
    "    print csv_file.readline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "也可以使用**readlines**方法一次读入所有内容："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "date,no1,no2,no3,no4,no5\n",
      "\n",
      "2014-01-01 00:00:00,-0.353731,0.285017,0.006235,-0.742147,0.004483\n",
      "\n",
      "2014-01-01 01:00:00,0.518702,0.150504,-0.565756,0.223530,0.694779\n",
      "\n",
      "2014-01-01 02:00:00,-0.256681,-0.403742,-0.463805,0.082921,-0.981190\n",
      "\n",
      "2014-01-01 03:00:00,0.669372,0.390002,0.020442,0.221858,1.088725\n",
      "\n"
     ]
    }
   ],
   "source": [
    "csv_file = open(path + 'data.csv','r')\n",
    "content = csv_file.readlines()\n",
    "for line in content[:5]:\n",
    "    print line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['date,no1,no2,no3,no4,no5\\n',\n",
       " '2014-01-01 00:00:00,-0.353731,0.285017,0.006235,-0.742147,0.004483\\n',\n",
       " '2014-01-01 01:00:00,0.518702,0.150504,-0.565756,0.223530,0.694779\\n',\n",
       " '2014-01-01 02:00:00,-0.256681,-0.403742,-0.463805,0.082921,-0.981190\\n',\n",
       " '2014-01-01 03:00:00,0.669372,0.390002,0.020442,0.221858,1.088725\\n']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "csv_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1.3 SQL数据库\n",
    "**Python**默认自带**SQLite3**数据库，利用这个数据库说明**Python**处理**SQL**数据库的方式："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sqlite3 as sq3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SQL**查询用字符串对象表示，语法、数据类型等取决于使用的数据库："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "query = 'create table numbs(date date,no1 real,no2 real)'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "打开一个数据库连接，本例中，在磁盘上生成一个新的数据库文件："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "con = sq3.connect(path + 'numbs.db')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后使用**execute**方法执行查询语句，创建一个表,并调用**commit**方法使查询生效："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlite3.Cursor at 0xb9a8030>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "con.execute(query)\n",
    "con.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以在表中填入数据，每个数据行由日期-时间信息和两个浮点数组成："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlite3.Cursor at 0xb9a8110>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime as dt\n",
    "#可以用单独的SQL语句写入单一数据行\n",
    "con.execute('insert into numbs values(?,?,?)',\n",
    "           (dt.datetime.now(),0.12,7.3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'2017-12-10 15:53:09.393000', 0.12, 7.3)]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=con.execute('select * from numbs').fetchmany(10)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#批量写入较大的数据集\n",
    "data = np.random.standard_normal((10000,2)).round(5)\n",
    "for row in data:\n",
    "    con.execute('insert into numbs values(?,?,?)',\n",
    "                (dt.datetime.now(),row[0],row[1]))\n",
    "con.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'2017-12-10 15:53:09.393000', 0.12, 7.3),\n",
       " (u'2017-12-10 15:58:50', 0.7961, 1.58976),\n",
       " (u'2017-12-10 15:58:50.005000', 0.95556, -2.82312),\n",
       " (u'2017-12-10 15:58:50.005000', -1.7089, 0.41756),\n",
       " (u'2017-12-10 15:58:50.005000', 0.96038, -0.49005),\n",
       " (u'2017-12-10 15:58:50.005000', 0.29436, 1.73957),\n",
       " (u'2017-12-10 15:58:50.005000', -0.35771, 1.73986),\n",
       " (u'2017-12-10 15:58:50.005000', 0.93397, -1.46124),\n",
       " (u'2017-12-10 15:58:50.005000', 0.52877, 0.133),\n",
       " (u'2017-12-10 15:58:50.005000', 0.02756, 0.80837)]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#从数据库读取多行\n",
    "con.execute('select * from numbs').fetchmany(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(u'2017-12-10 15:53:09.393000', 0.12, 7.3)\n",
      "(u'2017-12-10 15:58:50', 0.7961, 1.58976)\n",
      "(u'2017-12-10 15:58:50.005000', 0.95556, -2.82312)\n"
     ]
    }
   ],
   "source": [
    "#从数据库读取一行\n",
    "pointer = con.execute('select * from numbs')\n",
    "for i in range(3):\n",
    "    print pointer.fetchone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "OperationalError",
     "evalue": "unable to close due to unfinalized statements or unfinished backups",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-74-a1c69ccdb984>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcon\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mOperationalError\u001b[0m: unable to close due to unfinalized statements or unfinished backups"
     ]
    }
   ],
   "source": [
    "con.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1.4 读写NumPy数组\n",
    "**NumPy**本身有以便利、高性能的方式写入和读取ndarray对象的函数。这在某些情况下节省了工作量，例如必须将**NumPy dtype**转换为特定数据库类型时。为了说明**NumPy**有时候时基于**SQL**方法的高效替代品，重复前面的例子，这次仅用**NumPy**："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用**NumPy**的**arange**函数替代**pandas**，生成保存日期时间（datetime）对象的数组对象："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3681360"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtimes = np.arange('2015-01-01 10:00:00','2021-12-31 22:00:00',dtype = 'datetime64[m]')\n",
    "#minute intervals\n",
    "len(dtimes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SQL**数据库中的表是**NumPy**中的结构数组，我们使用特殊的**dtype**对象镜像前面的**SQL**表："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([('1970-01-01T00:00',  0.,  0.), ('1970-01-01T00:00',  0.,  0.),\n",
       "       ('1970-01-01T00:00',  0.,  0.), ..., ('1970-01-01T00:00',  0.,  0.),\n",
       "       ('1970-01-01T00:00',  0.,  0.), ('1970-01-01T00:00',  0.,  0.)],\n",
       "      dtype=[('Date', '<M8[m]'), ('no1', '<f4'), ('no2', '<f4')])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dty = np.dtype([('Date','datetime64[m]'),('no1','f'),('no2','f')])\n",
    "data = np.zeros(len(dtimes),dtype=dty)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "用**日期（dates）**对象填充**Date**列："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data['Date'] = dtimes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其他两列和以前一样用伪随机数填充："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = np.random.standard_normal((len(dtimes),2)).round(5)\n",
    "data['no1']=a[:,0]\n",
    "data['no2']=a[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ndarray**对象的保存经过了高度优化，因此相当快速，在磁盘保存数据："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 386 ms\n"
     ]
    }
   ],
   "source": [
    "%time np.save(path+'array',data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "读取数据更快："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 35 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([('2015-01-01T10:00',  1.36756003,  0.24412   ),\n",
       "       ('2015-01-01T10:01',  0.53118998,  0.68303001),\n",
       "       ('2015-01-01T10:02',  0.72872001,  0.43213999), ...,\n",
       "       ('2021-12-31T21:57', -0.99317998, -1.26206994),\n",
       "       ('2021-12-31T21:58', -1.53144002, -0.26495001),\n",
       "       ('2021-12-31T21:59',  0.51980001,  1.54949999)],\n",
       "      dtype=[('Date', '<M8[m]'), ('no1', '<f4'), ('no2', '<f4')])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time np.load(path + 'array.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在任何情况下都可预期，这种形式的数据存储和检索远快于**SQL**数据库或者使用标准**pickle**库的序列化。当然，使用这种方法当然没有**SQL**数据库的功能性，但是后面介绍的**PyTable**对此有所帮助。\n",
    "## 7.2 Pandas的I/O\n",
    "**pandas**库的主要优势之一是可以原生读取和写入不同数据格式，包括：CSV/SQL/XLS/XSLX/JSON/HTML等。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2.1 SQL数据库\n",
    "### 7.2.2 从SQL到pandas\n",
    "### 7.2.3 CSV文件数据\n",
    "### 7.2.4 Excel文件数据\n",
    "## 7.3 PyTable的快速I/O\n",
    "### 7.3.1 使用表\n",
    "### 7.3.2 使用压缩表\n",
    "### 7.3.3 使用数组\n",
    "### 7.3.4 内存外计算\n",
    "## 7.4 结语\n",
    "## 7.5 延伸阅读"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
